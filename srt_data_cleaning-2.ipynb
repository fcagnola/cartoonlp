{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "srt_data_cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxYBOneIQCxZ"
      },
      "source": [
        "!pip install nltk pysrt numpy pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DopNve2Qsm2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822f27e1-dc4f-4833-ad74-25e56c9bb850"
      },
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import pysrt\n",
        "import pickle\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4nYwm37TCuY"
      },
      "source": [
        "# Create final object\n",
        "subtitles = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8RfNR0gSJ6M"
      },
      "source": [
        "# Gather all ads and phrases used to promote the subtitles platform\n",
        "opensubs_ads = r'(Advertise your product or brand here)|(contact www\\.OpenSubtitles\\.(org|com) today)|(Support us and become VIP member)|(to remove all ads from www\\.OpenSubtitles\\.(org|com))|(-== \\[ www\\.OpenSubtitles\\.(org|com) \\] ==-)|(((Subtitles by )|(Sync by ))(.+?)$)|(font color=\\\"(.+)?\\\")|(Provided by(.+)$)|(^(https?):\\/\\/[^\\s\\/$.?#].[^\\s]*$)|(Please rate this subtitle at (.)*$)|(Help other users to choose the best subtitles)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB4gbGpASE_z"
      },
      "source": [
        "# Loop through files, take the text, remove all ads and add it to the object\n",
        "for file in os.listdir():\n",
        "  if not file.startswith(\".\"):\n",
        "    srt = pysrt.open(file)\n",
        "    remove_ads = re.sub(re.compile(opensubs_ads), \" \", srt.text)\n",
        "    year = file.split(\"_\")[-1].strip(\".srt\")\n",
        "    title = \"_\".join(file.split(\"_\")[:-1])\n",
        "    subtitles[title] = [year, remove_ads]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx4GUMDrTAOR"
      },
      "source": [
        "# Remove newline characters, braces ([{<>}]) and compact resulting whitespace \n",
        "for subtitle in subtitles:\n",
        "  text = subtitles[subtitle][1]\n",
        "  remove_newlines = re.sub(re.compile(r\"\\n+\"), \" \", text)\n",
        "  remove_braces = re.sub(re.compile(r\"(<\\/?\\w+(\\s\\w+=(.)+?)?>)|(\\[.*?\\])|\\{.+?\\}|\\(.+?\\)|â™ª\"), \" \", remove_newlines)\n",
        "  compact_whitespace = re.sub(re.compile(r\"\\s+\"), \" \", remove_braces)\n",
        "  subtitles[subtitle][1] = compact_whitespace\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDypTSz6TfqG",
        "outputId": "edd229b3-6103-4f20-b25c-9096a3ef56b0"
      },
      "source": [
        "df = pd.DataFrame.from_dict(subtitles, orient='index', columns=[\"Year\", \"Text\"])\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Year                                               Text\n",
            "Meet_the_Robinsons        2007  Then I didn't choose that one because it was g...\n",
            "Saludos_Amigos            1942   Saludos Amigos A fond greeting to you A warm ...\n",
            "Sleeping_Beauty           1959   I know you I waked with you once upon a dream...\n",
            "The_Aristocats            1970   Which pet's address is the finest in Paris? W...\n",
            "The_Emperor's_New_Groove  2000  Will you take a look at that? Pretty pathetic,...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWZwlEJBu3rt"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9nXPheZvAJH"
      },
      "source": [
        "def sentence_tokenize(txt: str):\n",
        "    \"\"\"Tokenize sentences: split text into sentences\"\"\"\n",
        "    return sent_tokenize(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qfD2CpruuSB",
        "outputId": "9cff0b32-f797-4138-da0f-3f921e97fb2a"
      },
      "source": [
        "df['Sentence_Tokenized'] = df['Text'].apply(sentence_tokenize)\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Year  ...                                 Sentence_Tokenized\n",
            "Meet_the_Robinsons        2007  ...  [Then I didn't choose that one because it was ...\n",
            "Saludos_Amigos            1942  ...  [ Saludos Amigos A fond greeting to you A warm...\n",
            "Sleeping_Beauty           1959  ...  [ I know you I waked with you once upon a drea...\n",
            "The_Aristocats            1970  ...  [ Which pet's address is the finest in Paris?,...\n",
            "The_Emperor's_New_Groove  2000  ...  [Will you take a look at that?, Pretty patheti...\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQXpY2Egvhzv"
      },
      "source": [
        "from numpy import array_split\n",
        "df['50_chunks'] = df['Sentence_Tokenized'].apply(lambda x: array_split(x, 50))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvoXtRPfzXO0",
        "outputId": "13b35526-00d4-4e4e-90b2-bcff1daabb4f"
      },
      "source": [
        "df['Word_Tokenized'] = df['Text'].apply(word_tokenize)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    }
  ]
}